{"cells":[{"cell_type":"markdown","source":["# Assignment Solution 1: Analysis of Power Usage Data"],"metadata":{}},{"cell_type":"markdown","source":["#### Instructions \n\nWe will use a dataset from one smartmeter to analyze the energy consumption pattern of a house. Analyze the electricity usage pattern and see what conclusion you can draw about the resident?\n\nNote:\n1. You need to pay attention to missing data;\n2. calculate some aggregate values of energy usage and observe different type of trends (e.g. pattern in a day, pattern in a week, pattern in a year, etc);\n3. Use Spark to do your calculations, then use dataframes to draw some plots. Describe each plot briefly and draw some conclusions;\n4. You only need to use the simple Spark transformations and actions covered in class, no need to use machine learning methods yet."],"metadata":{}},{"cell_type":"markdown","source":["#### Description of the Dataset\nSource: https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption#\n\nThis archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months). \n\nNotes: \n\n1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3. \n\n2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n\n\nAttribute Information:\n\n1.date: Date in format dd/mm/yyyy \n\n2.time: time in format hh:mm:ss \n\n3.global_active_power: household global minute-averaged active power (in kilowatt) \n\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt) \n\n5.voltage: minute-averaged voltage (in volt) \n\n6.global_intensity: household global minute-averaged current intensity (in ampere) \n\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). \n\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. \n\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner."],"metadata":{}},{"cell_type":"markdown","source":["#### Load the data into an RDD"],"metadata":{}},{"cell_type":"code","source":["#read txt file, gzipped files will be auto-unzipped\nmyRDD = sc.textFile(\"/mnt/mlonspark/household_power_consumption.txt.gz\")\nprint myRDD.count()\nmyRDD.take(5)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["#### Load Data to Spark Dataframe and describe data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\ntuples = myRDD.map(lambda x:x.split(\";\")).map(lambda x:[x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8]])\nfields=[\"dt\", \"time\", \"global_active_power\", \"global_reactive_power\", \"voltage\", \"global_intensity\", \"sub_metering_1\", \"sub_metering_2\", \"sub_metering_3\"]\norigDF=sqlContext.createDataFrame(tuples, fields)\norigDF.show(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### Data Pre-processing & Transformation"],"metadata":{}},{"cell_type":"code","source":["from datetime import datetime\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DateType\n\nfunc =  udf (lambda x: datetime.strptime(x, '%d/%m/%Y'), DateType())\ndf = origDF.withColumn('date', func(col('dt')))\ndf.printSchema()\ndf.show(5)\ndf.createOrReplaceTempView(\"temp\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["#### Active Power Consumption - Yearly Trend"],"metadata":{}},{"cell_type":"code","source":["active_power_consumption_per_year_sum=spark.sql(\"select year(date) as year, round(sum(global_active_power)) as active_power from temp group by year order by year\")\ndisplay(active_power_consumption_per_year_sum)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["**Analysis : **\n \n \n 1. As the dataset covers for only 1 month (Decemeber) in 2006, hence the power consumption shows very less compared to other years.\n 2. Similarly, for 2010, the dataset exists for only 11 months (until November), hence power consumption is slightly less compared to other year.\n 3. Excluding 2006 and 2010, for all other years, power consumption remanins stable."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F\ndef myConcat(*cols): return F.concat(*[F.coalesce(c, F.lit(\"*\")) for c in cols])\nmonthdf=dfyear.withColumn(\"yearmonth\", myConcat(\"year\", \"month\")).drop(\"year\").drop(\"month\") \nmonthdf.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["#### Active Power Consumption - Monthly Trend in Each Year"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F\ndef myConcat(*cols): return F.concat(*[F.coalesce(c, F.lit(\"*\")) for c in cols])\n\nyrmonthDF=spark.sql(\"select year(date) as year, month(date) as month, round(sum(global_active_power)) as active_power from temp group by year,month order by year,month\")\nyrmonthDF.show(5)\nactive_power_consumption_per_year_month=yrmonthDF.withColumn(\"yearmonth\", myConcat(\"year\", \"month\")).drop(\"year\").drop(\"month\") \nactive_power_consumption_per_year_month.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(active_power_consumption_per_year_month)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["**Analysis**\n \n \n 1. <TBD>"],"metadata":{}},{"cell_type":"markdown","source":["#### Active Power Consumption - By Month"],"metadata":{}},{"cell_type":"code","source":["active_power_consumption_per_month_sum=spark.sql(\"select month(date) as month, round(sum(global_active_power)) as active_power from temp group by month order by month\")\ndisplay(active_power_consumption_per_month_sum)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["**Analysis**\n \n \n 1. <TBD>"],"metadata":{}},{"cell_type":"markdown","source":["#### Active Power Consumption - Weekly Trend"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format\nweekday=spark.sql(\"select date_format(date,'E') as weekday, round(sum(global_active_power)) as active_power from temp group by weekday order by active_power desc \")\nweekday.show(10)\ndisplay(weekday)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["**Analysis**\n \n \n 1. <TBD>"],"metadata":{}},{"cell_type":"markdown","source":["#### Active Power Consumption - Daily Trend"],"metadata":{}},{"cell_type":"code","source":["active_power_consumption_per_day_sum=spark.sql(\"select date, round(sum(global_active_power)) as active_power from temp group by date order by date\")\ndisplay(active_power_consumption_per_day_sum)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["**Analysis**\n \n \n 1. <TBD>"],"metadata":{}}],"metadata":{"name":"Assignment1-Analysis of Power Consumption Data","notebookId":264944189010086},"nbformat":4,"nbformat_minor":0}
